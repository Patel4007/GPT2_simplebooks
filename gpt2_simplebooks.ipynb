{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"textminr/simplebooks\", split=\"train[:1%]\")  \ntexts = dataset[\"text\"]  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:34:55.989437Z","iopub.execute_input":"2025-02-15T20:34:55.989842Z","iopub.status.idle":"2025-02-15T20:35:09.530744Z","shell.execute_reply.started":"2025-02-15T20:34:55.989802Z","shell.execute_reply":"2025-02-15T20:35:09.529881Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e0c27d80ea0442dbe39ecee360e9290"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"simplebooks.py:   0%|          | 0.00/4.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29827120ede44df5a953ae7c1d01d61c"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for textminr/simplebooks contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/textminr/simplebooks.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/282M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9845031345433095851eea10291614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d66146877464e60b6d4007de23c73b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcf346f01ca41dcbfffee9df0b33094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0f2cc0b932a419688ac4a67bdc0cd9c"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=512)\n\ntokenizer.pad_token = tokenizer.eos_token\n\ninputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n\nlabels = inputs[\"input_ids\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:35:13.494585Z","iopub.execute_input":"2025-02-15T20:35:13.494919Z","iopub.status.idle":"2025-02-15T20:35:34.804802Z","shell.execute_reply.started":"2025-02-15T20:35:13.494890Z","shell.execute_reply":"2025-02-15T20:35:34.804114Z"}},"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e5fd650fcd142c48c120eed311413fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3724d343f8f4d8cb2424db73538c687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4363964593464d4ebc1cf73d03068d88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c62eab79d2e44e258cba97f35cef17ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8824a0add584638ba7593d21dba093b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca0906d60d3344d097d8bc7e2b75e547"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n\noptimizer = Adam(learning_rate=5e-5)\n\n# Function for the training step\n\n@tf.function\ndef train_step(input_ids, labels):\n    with tf.GradientTape() as tape:\n\n        # Forward Pass\n        \n        outputs = model(input_ids, labels=labels)\n        loss = outputs.loss\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    return loss\n\n\nbatch_size = 2  \ndataset = tf.data.Dataset.from_tensor_slices((inputs[\"input_ids\"], labels)).batch(batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T20:37:00.080077Z","iopub.execute_input":"2025-02-15T20:37:00.080388Z","iopub.status.idle":"2025-02-15T20:37:01.055675Z","shell.execute_reply.started":"2025-02-15T20:37:00.080364Z","shell.execute_reply":"2025-02-15T20:37:01.054822Z"}},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n\nAll the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"epochs = 5\nfor epoch in range(epochs):\n    total_loss = 0\n    print(f\"Epoch {epoch+1}/{epochs}\", flush=True)\n    for batch in dataset:\n        input_ids, labels = batch\n        loss = train_step(input_ids, labels)\n\n        total_loss += loss.numpy()  \n\nmodel.save_pretrained(\"distilgpt2_finetuned_simplebooks\")\ntokenizer.save_pretrained(\"distilgpt2_finetuned_simplebooks\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:01:32.523784Z","iopub.execute_input":"2025-02-15T21:01:32.524129Z","iopub.status.idle":"2025-02-15T21:07:36.605844Z","shell.execute_reply.started":"2025-02-15T21:01:32.524097Z","shell.execute_reply":"2025-02-15T21:07:36.605100Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\nEpoch 2/5\nEpoch 3/5\nEpoch 4/5\nEpoch 5/5\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('distilgpt2_finetuned_simplebooks/tokenizer_config.json',\n 'distilgpt2_finetuned_simplebooks/special_tokens_map.json',\n 'distilgpt2_finetuned_simplebooks/vocab.json',\n 'distilgpt2_finetuned_simplebooks/merges.txt',\n 'distilgpt2_finetuned_simplebooks/added_tokens.json')"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Model Inference","metadata":{}},{"cell_type":"code","source":"input_text = \"Once upon a time\"\ninputs = tokenizer(input_text, return_tensors=\"tf\")\n\ngenerated_ids = model.generate(inputs['input_ids'], max_length=200, num_return_sequences=1)\n\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:22:37.593958Z","iopub.execute_input":"2025-02-15T21:22:37.594328Z","iopub.status.idle":"2025-02-15T21:22:39.325206Z","shell.execute_reply.started":"2025-02-15T21:22:37.594300Z","shell.execute_reply":"2025-02-15T21:22:39.324453Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Once upon a time a king gave a holiday to all the people in one of his cities .'\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"input_text = \"There is a\"\ninputs = tokenizer(input_text, return_tensors=\"tf\")\n\ngenerated_ids = model.generate(inputs['input_ids'], max_length=200, num_return_sequences=1)\n\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:24:37.487859Z","iopub.execute_input":"2025-02-15T21:24:37.488200Z","iopub.status.idle":"2025-02-15T21:24:42.282339Z","shell.execute_reply.started":"2025-02-15T21:24:37.488172Z","shell.execute_reply":"2025-02-15T21:24:42.281482Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"There is a little trick here . The trick was finding the Striped Cat . He took a big bag of toys , and put down the Cat . He took a big bag of toys , and put it on , tying it to the tree .'\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"input_text = \"There were\"\ninputs = tokenizer(input_text, return_tensors=\"tf\")\n\ngenerated_ids = model.generate(inputs['input_ids'], max_length=200, num_return_sequences=1)\n\ngenerated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n\n\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T21:27:03.978757Z","iopub.execute_input":"2025-02-15T21:27:03.979065Z","iopub.status.idle":"2025-02-15T21:27:07.058383Z","shell.execute_reply.started":"2025-02-15T21:27:03.979017Z","shell.execute_reply":"2025-02-15T21:27:07.057645Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"There were some of the jolly workmen in the shop . They would fix the glass and glass jugs so that they would never do anything wrong .'\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"import transformers\n\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:55:15.729470Z","iopub.execute_input":"2025-02-16T22:55:15.729786Z","iopub.status.idle":"2025-02-16T22:55:20.914408Z","shell.execute_reply.started":"2025-02-16T22:55:15.729763Z","shell.execute_reply":"2025-02-16T22:55:20.913351Z"}},"outputs":[{"name":"stdout","text":"4.47.0\n","output_type":"stream"}],"execution_count":2}]}